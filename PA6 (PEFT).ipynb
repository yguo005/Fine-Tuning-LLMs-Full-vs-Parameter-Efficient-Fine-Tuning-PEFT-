{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc30363",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bbda16",
   "metadata": {
    "id": "78bbda16"
   },
   "source": [
    "In this exercise, you will fine-tune the [Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model for enhanced dialogue summarization. You will first explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter-Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38df4f",
   "metadata": {
    "id": "ae38df4f"
   },
   "source": [
    "## 1. Set up Dependencies and Load Dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa9a71",
   "metadata": {
    "executionInfo": {
     "elapsed": 10949,
     "status": "ok",
     "timestamp": 1712127151287,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "cfaa9a71"
   },
   "outputs": [],
   "source": [
    "!pip install datasets evaluate rouge_score peft -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc5e61",
   "metadata": {
    "executionInfo": {
     "elapsed": 15489,
     "status": "ok",
     "timestamp": 1712127166774,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "7bfc5e61"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0022db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4131,
     "status": "ok",
     "timestamp": 1712127170891,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "9e0022db",
    "outputId": "f32b33d9-c9ab-46d1-e7a1-6221e19e4f66"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('knkarthick/dialogsum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a483e",
   "metadata": {
    "id": "829a483e"
   },
   "source": [
    "Load the pre-trained [Flan-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of Flan-T5. Setting `torch_dtype=torch.bfloat16` specifies the data type to be used by this model, which can reduce GPU memory usage since `bfloat16` uses half as much memory per number compared to `float32`, the default precision for most models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b475615",
   "metadata": {
    "executionInfo": {
     "elapsed": 3570,
     "status": "ok",
     "timestamp": 1712127174457,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "3b475615"
   },
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404331c9",
   "metadata": {
    "id": "404331c9"
   },
   "source": [
    "## 2. Test the Model with Zero-Shot Inferencing\n",
    "\n",
    "Test the model with zero-shot inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903afec6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4001,
     "status": "ok",
     "timestamp": 1712127178454,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "903afec6",
    "outputId": "208a7ae1-b77b-41ff-c7a9-5415c63f1154"
   },
   "outputs": [],
   "source": [
    "index = 42\n",
    "dash_line = '-' * 100\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
    "original_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{original_model_summary}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hqKrbmjxvXhn",
   "metadata": {
    "id": "hqKrbmjxvXhn"
   },
   "source": [
    "You can see that the model struggles to summarize the dialogue compared to the baseline summary, and simply repeats the first sentence from the dialogue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39269c1c",
   "metadata": {
    "id": "39269c1c"
   },
   "source": [
    "## 3. Perform Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb795b4",
   "metadata": {
    "id": "ebb795b4"
   },
   "source": [
    "### 3.1 Preprocess the Dataset\n",
    "\n",
    "You need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation.`, and to the start of the summary with `Summary:` as follows:\n",
    "\n",
    "Training prompt (dialogue):\n",
    "```\n",
    "Summarize the following conversation.\n",
    "Alice: This is her part of the conversation.\n",
    "Bob: This is his part of the conversation.    \n",
    "Summary:\n",
    "```\n",
    "\n",
    "Training response (summary):\n",
    "```\n",
    "Both Alice and Bob participated in the conversation.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EVGHecAnv8s0",
   "metadata": {
    "id": "EVGHecAnv8s0"
   },
   "source": [
    "**Exercise**: Write a function to tokenize a batch of examples from the dialogue dataset. The function should concatentate the dialogues with the predefined prompt, tokenize them along with their summaries, and define the tokenized summaries as the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T6bMPv3ZK3UD",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1712127178454,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "T6bMPv3ZK3UD"
   },
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    \n",
    "    ## Create the input prompts by concatenating dialogue with the predefined instruction\n",
    "    inputs = []\n",
    "    for dialogue in examples['dialogue']:\n",
    "        prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\"\n",
    "        inputs.append(prompt)\n",
    "\n",
    "    # Tokenize the input prompts\n",
    "\n",
    "    #Truncate sequences that are too long\n",
    "    #Pad shorter sequences to match the longest in the batch\n",
    "    #return_tensors=None: Return as lists rather than tensors (required for dataset mapping)\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=True, return_tensors=None)\n",
    "\n",
    "    # Tokenize the target summaries (labels)\n",
    "    targets = tokenizer(examples['summary'], truncation=True, padding=True, return_tensors=None)\n",
    "    \n",
    "    # Set the labels to the tokenized summaries\n",
    "\n",
    "    # labels are the target outputs that want the model to learn to generate. They represent the \"correct answers\" during training.\n",
    "    model_inputs['labels'] = targets['input_ids']\n",
    "    \n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JPtVA3XzK5OG",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1712127178454,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "JPtVA3XzK5OG"
   },
   "outputs": [],
   "source": [
    "## Apply preprocessing to entire dataset\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101bb5da",
   "metadata": {
    "id": "101bb5da"
   },
   "source": [
    "### 3.2 Fine-Tune the Model\n",
    "\n",
    "**Exercise**: Utilize the Hugging Face Trainer API for training the model on the preprocessed dataset. Define the training arguments, a data collator, and create a `Seq2SeqTrainer` instance. Train the model for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8f449",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1712127179158,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "1ad8f449",
    "outputId": "8bdc1f04-200d-44cc-acff-e02b3fb3c5e2"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./flan-t5-base-dialogsum',          # Directory to save model checkpoints\n",
    "    num_train_epochs=1,                             # Train for 1 epoch as specified\n",
    "    per_device_train_batch_size=8,                  # Batch size per device\n",
    "    per_device_eval_batch_size=8,                   # Evaluation batch size\n",
    "    warmup_steps=500,                               # Number of warmup steps\n",
    "    weight_decay=0.01,                              # regularization technique that adds a penalty term to the loss function to prevent overfitting by penalty for large weights\n",
    "    logging_dir='./logs',                           # Directory for storing logs\n",
    "    logging_steps=100,                              # Log every 100 steps\n",
    "    eval_strategy=\"epoch\",                    # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                          # Save checkpoint at the end of each epoch\n",
    "    predict_with_generate=True,                     # Use generate for evaluation\n",
    "    bf16=True,                                      # Use BFloat16 mixed precision training (compatible with bfloat16 model)\n",
    "    fp16=False,                                     # Disable FP16 since we're using BF16\n",
    "    push_to_hub=False,                              # Don't push to hub\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "## Create data collator\n",
    "\n",
    "# Handles padding and batching for sequence-to-sequence tasks, Creates attention masks to ignore padding tokens\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=original_model,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b86e4",
   "metadata": {
    "id": "070b86e4"
   },
   "source": [
    "Training a fully fine-tuned version of the model should take about 10 minutes on a Google Colab GPU machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e44303",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "executionInfo": {
     "elapsed": 669903,
     "status": "ok",
     "timestamp": 1712127849649,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "22e44303",
    "outputId": "dbdc2866-1ccc-42d5-82ca-acc8f9ab2c6e"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcOg9qQ_9M5M",
   "metadata": {
    "id": "bcOg9qQ_9M5M"
   },
   "source": [
    "Save the model to a local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "klGQxAQf7prf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1865,
     "status": "ok",
     "timestamp": 1712127851511,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "klGQxAQf7prf",
    "outputId": "ef1c9ec5-0a50-4d5f-df8c-23012f0ae542"
   },
   "outputs": [],
   "source": [
    "model_path = './flan-t5-base-dialogsum-checkpoint'\n",
    "\n",
    "original_model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814042dd",
   "metadata": {
    "id": "814042dd"
   },
   "source": [
    "Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f4126",
   "metadata": {
    "executionInfo": {
     "elapsed": 2029,
     "status": "ok",
     "timestamp": 1712127853538,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "d98f4126"
   },
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained('./flan-t5-base-dialogsum-checkpoint', \n",
    "                                                       torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dK1EsTihBrHJ",
   "metadata": {
    "id": "dK1EsTihBrHJ"
   },
   "source": [
    "Reload the original Flan-T5-base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mdS7JS6PBpAh",
   "metadata": {
    "executionInfo": {
     "elapsed": 2735,
     "status": "ok",
     "timestamp": 1712127856268,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "mdS7JS6PBpAh"
   },
   "outputs": [],
   "source": [
    "original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964bc7e3",
   "metadata": {
    "id": "964bc7e3"
   },
   "source": [
    "### 3.3 Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "**Exercise**: Make inferences for the same example as in Section 2, using the original model and the fully fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10df481",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6924,
     "status": "ok",
     "timestamp": 1712127863177,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "e10df481",
    "outputId": "4726456e-7801-48c9-b669-bf84753a64b6"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "index = 42\n",
    "dash_line = '-' * 100\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "# Create the prompt\n",
    "prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
    "\n",
    "#Generate summary with original model\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
    "original_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "#Generate summary with fine-tuned (instruct) model\n",
    "output = instruct_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
    "instruct_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "# Display results for comparison\n",
    "print(dash_line)\n",
    "print(f'INPUT DIALOGUE:\\n{dialogue}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL SUMMARY:\\n{original_model_summary}')\n",
    "print(dash_line)\n",
    "print(f'FINE-TUNED MODEL SUMMARY:\\n{instruct_model_summary}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65474b3",
   "metadata": {},
   "source": [
    "The fine-tuned model is able to create a much better summary of the dialogue compared to the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ffbd2",
   "metadata": {
    "id": "4b1ffbd2"
   },
   "source": [
    "### 3.4 Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "\n",
    "The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a480b",
   "metadata": {
    "executionInfo": {
     "elapsed": 1836,
     "status": "ok",
     "timestamp": 1712127865010,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "cf2a480b"
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "in2SYXRfCNWA",
   "metadata": {
    "id": "in2SYXRfCNWA"
   },
   "source": [
    "**Exercise**: Generate the outputs for a sample of the test set with the fine-tuned model (use only the first 10 dialogues and summaries to save time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pAOnB6lFCOUw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 105235,
     "status": "ok",
     "timestamp": 1712127970243,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "pAOnB6lFCOUw",
    "outputId": "4a58dff8-bed8-41f1-f28c-da222f2a412f"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# Generate summaries for the first 10 test examples\n",
    "dialogues = dataset['test']['dialogue'][:10]\n",
    "human_baseline_summaries = dataset['test']['summary'][:10]\n",
    "\n",
    "# Generate summaries with original model\n",
    "original_model_summaries = []\n",
    "for dialogue in dialogues:\n",
    "    prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt') # Tokenizer converts text to tokens:\n",
    "    output = original_model.generate(inputs['input_ids'],max_new_tokens=50)[0] #Model generates from token IDs, the generate() method needs the actual token IDs as input\n",
    "    summary = tokenizer.decode(output, skip_special_tokens=True) #tokenizer is a tool for converting between text and token\n",
    "    original_model_summaries.append(summary)\n",
    "\n",
    "# Generate summaries with fine-tuned (instruct) model\n",
    "instruct_model_summaries = []\n",
    "for dialogue in dialogues:\n",
    "    prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = instruct_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
    "    summary = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(summary)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a019aaa",
   "metadata": {
    "id": "4a019aaa"
   },
   "source": [
    "Evaluate the models computing ROUGE metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77847c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1114,
     "status": "ok",
     "timestamp": 1712127971350,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "d77847c0",
    "outputId": "d9db95bb-a8ff-43c3-a840-c661e05dc4d8"
   },
   "outputs": [],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)]\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)]\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44701b",
   "metadata": {
    "id": "1c44701b"
   },
   "source": [
    "The results show substantial improvement in all ROUGE metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1a51a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712127971350,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "e2d1a51a",
    "outputId": "df9e6682-82f6-4793-e410-3824041d4e4b"
   },
   "outputs": [],
   "source": [
    "print(\"Absolute percentage improvement of the instruct model over the original model:\")\n",
    "\n",
    "for key in instruct_model_results:\n",
    "    improvement = instruct_model_results[key] - original_model_results[key]\n",
    "    print(f'{key}: {improvement*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Iq72DeUafKOL",
   "metadata": {
    "id": "Iq72DeUafKOL"
   },
   "source": [
    "## 4. Perform Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "Now, let's perform **Parameter Efficient Fine-Tuning (PEFT)** instead of \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning, with comparable evaluation results as you will see soon.\n",
    "\n",
    "One of the most popular PEFT methods is **Low-Rank Adaptation (LoRA)**, which  introduces low-rank matrices to adapt the LLM with minimal additional parameters. In most cases, when someone says PEFT, they typically mean LoRA.  After fine-tuning for a specific task with LoRA, the result is that the original LLM remains unchanged and a newly-trained \"LoRA adapter\" emerges. This LoRA adapter is much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n",
    "\n",
    "At inference time, the LoRA adapter is reunited and combined with its original LLM to serve the inference request. The benefit is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jjMz_LZrfRKN",
   "metadata": {
    "id": "jjMz_LZrfRKN"
   },
   "source": [
    "### 4.1 Setup the LoRA model for Fine-Tuning\n",
    "\n",
    "You first need to define the configuration of the LoRA model. Have a look at the configuration below. The key configuration element to adjust is the rank (`r`) of the adapter, which influences its capacity and complexity. Experiment with various ranks, such as 8, 16, or 32, and see how they affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52eb8bf",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1712127971350,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "c52eb8bf"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1 #applied to the LoRA layers for regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "# LoRA decomposes the weight updates into two smaller matrices (A and B) that \n",
    "# when multiplied together approximate the full weight update matrix. \n",
    "# dapter Layers: Instead of modifying the original model weights (W), LoRA adds trainable low-rank matrices that compute: W + BA, where:\n",
    "#W = original frozen weights\n",
    "#B and A = small trainable matrices\n",
    "#The rank r determines the size of these matrices\n",
    "# Higher rank = more parameters = more capacity but also more memory\n",
    "#lora_alpha=32: The scaling factor that controls how much the LoRA adaptation affects the original model:\n",
    "#Formula: scaling = lora_alpha / r\n",
    "#With your values: scaling = 32/32 = 1.0\n",
    "#This means the LoRA updates have equal weight to the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lrxAQD2tflZA",
   "metadata": {
    "id": "lrxAQD2tflZA"
   },
   "source": [
    "Add LoRA adapter layers/parameters to the original LLM to be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1IHrKzPnfL-n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1712127971350,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "1IHrKzPnfL-n",
    "outputId": "83eefc20-b74a-455e-de03-38ab8293eb33"
   },
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ae88c",
   "metadata": {},
   "source": [
    "The number of trainable model parameters in the LoRA model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zpLzokfYfo_m",
   "metadata": {
    "id": "zpLzokfYfo_m"
   },
   "source": [
    "### 4.2 Train the LoRA Adapter\n",
    "\n",
    "**Exercise**: Define training arguments and create a `Seq2SeqTrainer` instance for the LoRA model. Use a higher learning rate than full fine-tuning (e.g., `1e-3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5oEIIiIofrC8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1712127971771,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "5oEIIiIofrC8",
    "outputId": "8d90fee9-a5d2-4fa0-aa01-892812d22642"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# Define training arguments for PEFT with higher learning rate\n",
    "peft_training_args= Seq2SeqTrainingArguments(\n",
    "    output_dir='./flan-t5-base-dialogsum-lora',     # Directory to save model checkpoints\n",
    "    num_train_epochs=1,                             # Train for 1 epoch\n",
    "    per_device_train_batch_size=8,                  # Batch size per device\n",
    "    per_device_eval_batch_size=8,                   # Evaluation batch size\n",
    "    warmup_steps=500,                               # Number of warmup steps\n",
    "    weight_decay=0.01,                              # Weight decay for regularization\n",
    "    logging_dir='./logs-peft',                      # Directory for storing logs\n",
    "    logging_steps=100,                              # Log every 100 steps\n",
    "    learning_rate=1e-3,                             # Higher learning rate for PEFT (1e-3 vs default ~5e-5)\n",
    "    eval_strategy=\"epoch\",                          # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                          # Save checkpoint at the end of each epoch\n",
    "    predict_with_generate=True,                     # Use generate for evaluation\n",
    "    bf16=True,                                      # Use BFloat16 mixed precision training\n",
    "    fp16=False,                                     # Disable FP16 since we're using BF16\n",
    "    push_to_hub=False,                              # Don't push to hub\n",
    "    report_to=[],  \n",
    ")\n",
    "\n",
    "# Create data collator for PEFT\n",
    "peft_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=peft_model,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# create PEFT trainer\n",
    "peft_trainer = Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=peft_data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_2jryxZgMdR",
   "metadata": {
    "id": "H_2jryxZgMdR"
   },
   "source": [
    "Train the PEFT adapter. Training should take about 6 minutes on a Google Colab GPU machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7T_P1gNlP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "executionInfo": {
     "elapsed": 607410,
     "status": "ok",
     "timestamp": 1712128579179,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "f0d7T_P1gNlP",
    "outputId": "8536929b-13e1-4e72-ccc2-44807fca7547"
   },
   "outputs": [],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vJ_h6KU2gcUf",
   "metadata": {
    "id": "vJ_h6KU2gcUf"
   },
   "source": [
    "Save the model to a local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7T9fwZ0NhOKC",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1712128579179,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "7T9fwZ0NhOKC"
   },
   "outputs": [],
   "source": [
    "peft_model.save_pretrained('./flan-t5-base-dialogsum-lora')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KmqvrFOrhVby",
   "metadata": {
    "id": "KmqvrFOrhVby"
   },
   "source": [
    "Load the PEFT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wit_mz3Vgh-V",
   "metadata": {
    "executionInfo": {
     "elapsed": 2081,
     "status": "ok",
     "timestamp": 1712128581258,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "Wit_mz3Vgh-V"
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "peft_model = AutoModelForSeq2SeqLM.from_pretrained('./flan-t5-base-dialogsum-lora')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dogDPmBBkuya",
   "metadata": {
    "id": "dogDPmBBkuya"
   },
   "source": [
    "Reload the original Flan-T5-base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EFur6v7nkItQ",
   "metadata": {
    "executionInfo": {
     "elapsed": 2729,
     "status": "ok",
     "timestamp": 1712128583985,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "EFur6v7nkItQ"
   },
   "outputs": [],
   "source": [
    "original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-AHhrS2lheOH",
   "metadata": {
    "id": "-AHhrS2lheOH"
   },
   "source": [
    "### 4.3 Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "**Exercise**: Make inferences for the same example as in Sections 2 and 3, using the original model, the fully fine-tuned model and the PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yGFEKSwAhXr6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12318,
     "status": "ok",
     "timestamp": 1712128596301,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "yGFEKSwAhXr6",
    "outputId": "2fe25b25-4b11-4990-ca54-32b96f49cf20"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "index = 42\n",
    "dash_line = '-' * 100\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "\n",
    "# create the prompt\n",
    "prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
    "\n",
    "# Generate summary with original model\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
    "original_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "# Generate summary with fine-tuned (instruct) model\n",
    "output = instruct_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
    "instruct_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "# generate summary with PEFT model\n",
    "output = peft_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
    "peft_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "print(f'INPUT DIALOGUE:\\n{dialogue}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL SUMMARY:\\n{original_model_summary}')\n",
    "print(dash_line)\n",
    "print(f'FINE-TUNED MODEL SUMMARY:\\n{instruct_model_summary}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL SUMMARY:\\n{peft_model_summary}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YzNd3ptflWeS",
   "metadata": {
    "id": "YzNd3ptflWeS"
   },
   "source": [
    "### 4.4 Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "\n",
    "**Exercise**: Generate the outputs for a sample of the test set with the PEFT model (use only the first 10 dialogues and summaries to save time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lkDE0mtrkD1K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "executionInfo": {
     "elapsed": 166367,
     "status": "ok",
     "timestamp": 1712128762657,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "lkDE0mtrkD1K",
    "outputId": "f10d40cc-5fea-45d5-9cf2-969d44db9c82"
   },
   "outputs": [],
   "source": [
    "# Generate summaries for the first 10 test examples with PEFT model\n",
    "dialogues = dataset['test']['dialogue'][:10]\n",
    "human_baseline_summaries = dataset['test']['summary'][:10]\n",
    "\n",
    "# Generate summaries with PEFT model\n",
    "peft_model_summaries = []\n",
    "for dialogue in dialogues:\n",
    "    prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = peft_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n",
    "    summary = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    peft_model_summaries.append(summary)\n",
    "\n",
    "print(f\"Generated {len(peft_model_summaries)} summaries with PEFT model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TF6MuatKmFQz",
   "metadata": {
    "id": "TF6MuatKmFQz"
   },
   "source": [
    "Compute ROUGE score for this subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bZmMVyDYmCDF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1165,
     "status": "ok",
     "timestamp": 1712128763817,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "bZmMVyDYmCDF",
    "outputId": "f2f98ed2-1895-4463-8580-392dab794638"
   },
   "outputs": [],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NViojzFXmRn1",
   "metadata": {
    "id": "NViojzFXmRn1"
   },
   "source": [
    "Notice, that PEFT model results are not too bad, while the training process was much easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C5CmTgHxmZBS",
   "metadata": {
    "id": "C5CmTgHxmZBS"
   },
   "source": [
    "Calculate the improvement of PEFT over the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X2JFTooimR_t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1712128763817,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "X2JFTooimR_t",
    "outputId": "3bf396bf-3a2c-4f63-9cba-322aba36ed88"
   },
   "outputs": [],
   "source": [
    "print(\"Absolute percentage improvement of the PEFT model over the original model:\")\n",
    "\n",
    "for key in peft_model_results:\n",
    "    improvement = peft_model_results[key] - original_model_results[key]\n",
    "    print(f'{key}: {improvement*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P2yCaFcWmbWh",
   "metadata": {
    "id": "P2yCaFcWmbWh"
   },
   "source": [
    "Now calculate the improvement of PEFT over a full fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nNLKL9fOmc0p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712128763817,
     "user": {
      "displayName": "Roi Yehoshua",
      "userId": "09736484780652603363"
     },
     "user_tz": 240
    },
    "id": "nNLKL9fOmc0p",
    "outputId": "62d248a7-1180-4e9e-bab6-7a199728adfb"
   },
   "outputs": [],
   "source": [
    "print(\"Absolute percentage improvement of the PEFT model over the instruct model:\")\n",
    "\n",
    "for key in peft_model_results:\n",
    "    improvement = peft_model_results[key] - instruct_model_results[key]\n",
    "    print(f'{key}: {improvement*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tpji7Bi6meQx",
   "metadata": {
    "id": "Tpji7Bi6meQx"
   },
   "source": [
    "You can see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58819c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
